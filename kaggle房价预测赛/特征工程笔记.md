# 特征工程笔记

## 1.数据预处理

我们需要知道，往往特征有可能会有以下几个问题：

1. 特征不属于同一量纲：特征的规格，scale往往不一样，如果要把特征放到一起比较，需要进行无量纲化处理
2. 信息冗余：有些信息我们不关注，比如只关注‘及格’或者‘不及格’，这时候就需要把分数进行二值化。无量纲化可以比较好的解决这问题。
3. 定性特征无法直接使用：比如说胖or中or不胖，这个时候需要使用哑编码（度热编码）把n个值扩充成n个特征。比如把胖or中or不胖扩充成三个特征。这样可以让线性模型处理一些非线性的量。
4. 存在缺失值：填充缺失值 
5. 信息利用率低：我们对定量变量多项式化或者进行数据转换都能达到这样的效果

### 1.1 数值规范化

### 1.1.1 无量纲化与标准化

用StandardScalar，核心思想是把数据转化为（0，1）的正态分布

### 1.1.2 区间缩放

用MinMaxScaler，核心思想是把数据分布放缩到 [ 0,1 ] 区间内

标准化更好的情况：后续需要使用距离来度量相似性，或者使用pca，lda这些需要分析协方差进行降维的时候。同时数据分布可以近拟为正态分布。

缩放更好的情况：不涉及距离度量，协方差分析的情况，同时数据不符合正态分布。比如图像处理将RGB图像转化成灰度图像的时候归一化比较好

### 1.1.3 对行向量归一化

用Normalizer

### 1.2 定量特征二值化

二值化的核心在于设置一个阈值，大于阈值的赋值1，小于赋值0

代码是使用Binarizer，Binarizer(threshold=3).fit_transform(data)

### 1.3 定性特征哑编码

对于没有次序要求的哑编码，使用OneHotEncoder

### 1.4 缺失值

用sklearn里的inputer

Imputer(missing_values=,strategy='',axis=).fit_transform(data)

### 1.5 数据变换

### 1.5.1 多项式变换

用PolynomialFeatures()

### 1.5.2 自定义函数变换

FunctionTransformer(np.log).fit_transform()

## 2. 特征选择

数据预处理完后，我们需要选择有意义的特征。通常有两个方面选择

1. 观察特征是否发散，如果不发散方差等于0，那么选不选这个样本基本都没差
2. 特征与目标的相关性。

根据特征选择的形式又可以分为以下三种：

- Filter：过滤法

  按照发散型或者相关性各个特征进行评分，设定阈值或者待选择阈值的个数来选择特征。不考虑后续使用的学习器。

- Wrapper：包装法

  考虑之后使用的学习器，根据目标函数的预测效果，每次选择若干特征或者排除若干特征。

- Embedded：嵌入法

  结合filter and wrapper。先使用机器学习的算法和模型进行训练得到各个特征的权值系数，根据系数从大到小选择特征

我们使用feature_selection库来进行特征选择

## 3.1 Filter

### 3.1.1 使用方差来过滤

使用feature_selection.VarianceThreshold

Variance_Threshold(3) 指按照3作为阈值来过滤

### 3.1.2 单变量特征选择

单变量特征选择每次取出一个特征进行测试，衡量特征与目标之间的关系，根据得分来扔掉不好的特征。对于回归和分类问题采用chi-squared test 对特征进行测试

1. chi-squared hypothesis test

   用的包是sklearn.feature_selection.SelectKBest

   Sklearn.feature_selection.chi2

   SelectKBest(chi2, k=2 ).fit_transform(data.data,data.target)

   k代表最多选几个特征

2. correlation coefficient

   from scipy.stats import pearsonr

### 3.1.3 距离相关系数

距离相关系数用来克服Pearson系数的局限性，如果距离相关系数为0，我们可以说这两个变量是相互独立的。

### 3.2 Wrapper

wrapper包装可以理解为我们选择部分特征加入学习器，用学习器包装起来看 效果好不好，然后根据效果选择或者排除若干特征。实际就是一个搜索最优特征的过程，是一个 优化问题。

### 3.2.1 递归特征消除法

递归消除使用一个基模型进行训练，每轮训练后消除若干特征，再基于新特征进行下一轮训练。使用sklearn库中的RFE类

RFE(estimator, n_features_to_select).fit_transform(data.data,data.target)

n_features_to_select指最后留下多少个相关的特征

### 3.3 Embedded

我们把所有特征都嵌入学习器来训练，最后根据系数从大到小来选择系数

### 3.3.1 基于惩罚项的特征选择法

from sklearn.feature_selection import SelectFromModel

SelectFromModel(LogisticRegression(penalty='l1',C=0.1)).fit_transfrom

### 3.3.2 基于树模型的特征选择法

也是用SelectFromModel

## 4. 降维方法

不带标签的我们用pca，带标签的我们用lda