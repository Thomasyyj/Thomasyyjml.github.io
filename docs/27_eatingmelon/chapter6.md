# 第六章：支持向量机

## 6.1 间隔与支持向量

支持向量机是一种经典的二分类模型，其模型核心在于：对于一个线性可分的数据集，找出距离最近正负样本间隔最大的划分超平面。在上一章感知机，我们曾经提到过对于线性可分的数据集，我们能找到无穷多个划分超平面，svm则求这些超平面最‘中间’的唯一一个。我们把最近正负样本到超平面的距离之和称作间隔（margin）。最近的这几个样本指代的特征向量叫做支持向量（support vector）

同时这里我们继续回顾一下超平面的概念：

n维空间的超平面：$（w^Tx+b=0,其中w,x\in R^n）$具有以下性质：

- 超平面方程不唯一（系数可以任意缩放）

- 法向量 $w$ 和位移项 $b$ 确定一个平面

- 法向量 $w$ 垂直于平面

- 法向量 $w$ 指向的空间是正样本空间，正空间的点代入式子为正

- 任意点 $x$ 到超平面的距离为
  $$
  r = \frac{|w^Tx+b|}{\|w\|}
  $$
  简单证明过程如下:

  令$x_0$在超平面上的投影为$x_1$，则
  $$
  |w\cdot \vec{x_1x_0}|  =\|w\|\cdot|r|
  $$

  $$
  同时又有：|w\cdot \vec{x_1x_0}|=|w^T\vec{x_o}-w^T\vec{x_1}|=|w^Tx-b|
  $$

  $$
  联立两式可得：r = \frac{|w^Tx+b|}{\|w\|}
  $$

### 6.1.1 几何间隔

几何间隔就是代表上式提到的：数据点到超平面的距离。定义如下

对于给定的数据集$X$和超平面，数据集中的一个样本点 $(x_i,y_i),y_i\in \{-1,1\},i=1,2,\cdots,m$ 关于超平面的几何间隔为：
$$
\gamma_i = \frac{y_i|w^Tx_i+b|}{\|w\|}
$$
我们不难发现对于分类对的值，几何间隔为正，对于分类错误的值，几何间隔为负。

数据集到超平面的几何间隔为：
$$
\gamma = \min_{i=1,2,\cdots ,m}\gamma_i
$$
我们可以发现，这个模型有以下特点：

1. 如果有分类错误的点，几何间隔最小的点间隔小于0。所以我们模型首先需要满足分类正确
2. 当正确划分超平面后，间隔最大就代表我们找到的平面距离各个支持向量点都最远

### 6.1.2 支持向量机模型

于是我们的模型就出来了，给定线性可分的数据集$X$ ，支持向量机模型希望求得数据集X关于超平面的几何间隔$\gamma$最大的那个超平面，然后对求得的模型加一个sign函数来实现分类功能
$$
y=sign(w^T+b) = \begin{cases}
1 & w^Tx+b >0 \\
-1 & w^Tx+b <0
\end{cases}
$$

## 6.2 优化策略

给定线性可分数据集$X$，设$X$中的支持向量$(x_{min},y_{min})$，那么支持向量机找平面的问题能够等价于以下优化问题：
$$
max\  \gamma, such\ that\ \gamma_i >= \gamma,i=1,2,\cdots,m
$$
即：
$$
\max_{w,b}\frac{y_{min}|w^Tx_{min}+b|}{\|w\|}
$$

$$
s.t\ y_i(w^Tx_i+b)>=y_{min}(w^Tx_{min}+b)
$$

由于之前提到过超平面的性质，$w,b$可以自由放缩，因此这里我们令 $y_{min}|w^Tx_{min}+b|=1$ ,最终问题简化为：
$$
\max_{w,b}\frac{1}{\|w\|}
$$

$$
s.t.\ y_i(w^Tx_i+b)>=1
$$

$$
\min_{w,b}{\frac{1}{2}\|w\|^2}
$$

$$
s.t.\ 1-y_i(w^Tx_i+b)<=0
$$

 这是一个凸优化问题，这里用拉格朗日对偶法求解

### 6.2.1 拉格朗日对偶法

对于一般约束优化问题（不一定凸优化）
$$
min:f(x)\\s.t:\ g_i(x)<=0,i=1,2,\cdots,m(m个不等式约束)\\
h_j(x) = 0,j=1,2,\cdots,n(n个等式约束)
$$
设上述问题的定义域为$D$（$f(x),g_i(x),h_j(x)$所有定义域的交集）

可行域为$\hat{D}$为定义域内满足约束条件的集合

Lagrange函数为
$$
L(x,\mu,\lambda) = f(x) +\sum_{i=1}^m\mu_ig_i(x)+\sum_{j=1}^{n}\lambda_jh_j(x)
$$
where $\mu,\lambda$ is the Lagrange multiplier

Lagrange duality函数 $\Gamma(\mu,\lambda)$ 为Lagrange函数关于x的下确界
$$
\Gamma(\mu,\lambda)=\inf_{x\in D}L(x,\mu,\lambda)
$$
有如下性质：

- 无论优化问题是否为凸优化，对偶函数恒为凹函数

- 当$\mu >=0$时，$\Gamma(\mu,\lambda)$构成最优值$p^*$的下界，要求原问题的最小值转换成求$\Gamma(\mu,\lambda)$的最大值
  $$
  \Gamma(\mu,\lambda)<=p^*=min\{f(\hat{x})\}
  $$

因此原问题转换为
$$
max:\Gamma(\mu,\lambda)\\s.t.\ \mu >=0
$$
要使对偶函数的上界等于原式的下界，需要满足一些充分条件使强对偶性成立（5个kkt条件，具体见凸优化教材）。这里省略证明支持向量机满足Slater条件。这时无论主问题是否为凸优化问题，对偶问题恒为凸优化问题。

### 6.2.2 主问题优化求解

$$
\min_{w,b}\frac{1}{2}\|w\|^2\\s.t.:1-y_i(w^Tx_i+b)<=0,i=1,2,\cdots,m
$$

Lagrange function:
$$
L(w,b,\alpha) = \frac{1}{2}\|w\|^2+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b))
$$
把$w^Tx+b$合并成$\hat{w}$后，上式是关于$\hat{w}=(w;b)$的凸函数，直接求一阶导让其等于0，
$$
w=\sum_{i=1}^{m}\alpha_iy_ix_i,\\0=\sum_{i=1}^{m}\alpha_iy_i
$$
然后带回Lagrange函数即可消去$w$，再加上新的限制条件$0=\sum_{i=1}^{m}\alpha_iy_i$，即可得到对偶问题
$$
\max_{\alpha}\sum_{i=1}^{m}\alpha_i-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_i\\s.t.:\sum_{i=1}^{m}\alpha_iy_i=0
$$
求解对偶问题书上用的方法是SMO方法，这里先略因为启发式算法并没有太懂。。。之后补一下

为什么我们用对偶法来求解呢？毕竟主问题本来也是一个凸优化问题。答案是对偶问题我们用到的$\alpha$是特征个数，而直接主函数优化求导的是$w$数据个数，当数据量远大于特征个数时，显然求特征量的个数的导数更加方便。另一个原因是对偶问题可以引入核函数，推广到非线性分类问题。

## 6.4 软间隔与正则化

### 6.4.1 损失函数策略

在之前的讨论中都是基于样本是线性可分的情况。但是往往大部分情况未必完全是线性可分的，这时候我们就需要让模型能够接受一些错误从而防止过拟合。为了缓解该问题，我们需要引入‘软间隔’的概念让模型允许部分样本不满足下式子的约束条件。

我们这里把约束条件转化为损失函数，要求如下

- 当满足约束条件时，损失为0
- 当不满足约束条件时，损失不为0且损失与其违反约束条件的程度成正比（可选）

基于这些条件，优化目标改写为：
$$
\min_{w,b}\frac{1}{2}\|w\|^2+C\sum_{i=1}^{m}l_{0/1}(y_i(w^Tx_i+b)-1)\\where,l_{0/1}=\begin{cases}
1 & if\ z<0 \\
-1 & otherwise
\end{cases}
$$
如何理解？当样本分类不满足约束$y_i(w^Tx_i+b)\ge 1$时，我们计为出现1点损失，然后将所有损失乘以惩罚权重C即得到最后的损失函数。当惩罚权重C取到最大，则迫使学习器找到0损失的分类超平面，退化为硬间隔。

### 6.4.2 优化推导

在策略层面我们确定了损失函数，现在我们尝试求解该问题。

首先由于$l_{0/1}$函数非凸，非连续，数学性质不好，我们采用hinge损失函数来代替$l_{0/1}$
$$
hinge:l_{hinge}(z)=max(0,1-z)
$$
光理解这个函数可能不太好理解，我们先代入回原来的式子就好理解了
$$
\min_{w,b}\frac{1}{2}\|w\|^2+C\sum_{i=1}^{m}max(0,1-y_i(w^Tx_i+b))
$$
也就是说把损失函数计为：分类错误的点到约束区间$y_i(w^Tx_i+b)\ge 1$的距离与0的最大值。不满足约束的点到约束区间的距离越大，损失越大。当所有点都分类准确的时候，损失为0。

我们引入松弛变量$\xi_i$作为约束条件来求解：

令 $\xi_i = max(0,1-y_i(w^Tx_i+b)$，则显然有：
$$
\xi_i\ge 0\\y_i(w^Tx_i+b)\ge1-\xi_i
$$
优化问题变为：
$$
\min_{w,b,\xi_i}\frac{1}{2}\|w\|^2+C\sum_{i=1}^{m}\xi_i\\s.t.:\xi_i\ge 0\\\xi_i\ge1-y_i(w^Tx_i+b)
$$
直接理解也可以：$\xi_i$表示样本越界的值，损失函数就是加上各个点越界的值的加权和，同时约束条件是：越界值肯定大于0，越界值大于等于$1-y_i(w^Tx_i+b)$，因为一般情况是等于，分类对的话：$1-y_i(w^Tx_i+b)=0$就是大于。

那么同样还是写出拉格朗日函数，求一阶导消去$w$然后变成拉格朗日对偶问题求解。

### 6.4.3 损失函数的一般形式

在学习的过程中我发现一个问题，为什么hinge函数那么好理解，我们还要先那么复杂地定义$l_{0/1}$函数？实际上还有别的替换选项，如果我们用对率损失函数$log(1+e^{-z})$，我们会发现得到了几乎是逻辑回归的模型，实际上和逻辑回归的性能也相同。我们还可以把这个0/1损失函数替换成别的损失函数来得到别的学习模型，我们把这些损失函数的一般形式写为
$$
\min_{f}\Omega(f)+C\sum_{i=1}^{m}l(f(x_i),y_i)
$$
其中$\Omega(f)$称作结构风险，用于描述模型$f$的某些特质，$\sum_{i=1}^{m}l(f(x_i),y_i)$称作经验风险，用于描述模型与训练数据的契合程度。C用于调整对二者进行折中。从经验风险最小化的角度来看，$\Omega(f)$表述了我们希望得到什么性质的模型，复杂度低还是高。另一方面，它削减了假设空间，降低了过拟合的风险，从这个角度来说，$\Omega(f)$可以理解成正则项，这也是为什么之前推导的式子里$\|w\|^2$这么像L2正则项，实际上它就是。

L2范数倾向于分量尽可能均衡，非零分量个数尽可能稠密，因为l2范数的几何意义是圆。L1范数和L0范数倾向于分量尽可能稀疏，非零分量个数尽可能少。





