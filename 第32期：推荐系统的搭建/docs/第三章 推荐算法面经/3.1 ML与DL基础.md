# 3.1 机器学习相关

## 3.1.1 机器学习
- 介绍一个最熟悉的机器学习算法

- 介绍下GBDT

- 介绍XGBoost

- 介绍下LightGBM

- LightGBM相对于XGBoost的改进

- GBDT中的梯度是什么，怎么用

- GBDT如何计算特征重要性

- 介绍XGBoost中的并行

- 介绍XGBoost中精确算法与近似算法

- XGBoost如何处理空缺值，为何要进行行采样、列采样

- 为什么高维稀疏数据，LR比GBDT要好

- 随机森林与GBDT采样的区别

- 随机森林中列采样的作用

- bagging与boosting对比

- bagging与boosting分别从什么角度降低过拟合

- 逻辑回归如何避免过拟合

- 推导逻辑回归损失函数和损失函数求导

- 正则化项L1和L2为什么有用

- l1正则不可导，如何优化

- 什么样的特征容易产生比较小的权重

- 随机森林采样n次，n趋于无穷大，oob样本的概率接近于？

- 逻辑回归与树模型的优缺点

- 对于高维稀疏数据，树模型能训练吗？一般怎么处理

- 树模型一般有哪些参数，分别有什么作用

- 随机森林如何处理空缺值

- 介绍kmeans，与其他聚类算法的对比

- 机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？

- 如何解决过拟合问题？哪些角度

## 3.1.2 深度学习

- 优化器，SGD与Adam的异同点

- SGD缺点，已经有什么改进的优化器

- 网络权重初始化为0有什么影响，初始化为一个非0的常数呢？

- embedding如何设置维度？越大越好还是越小越好？

- transformer中计算attention除于根号d的作用

- embedding如何训练

- 介绍下attention，相比cnn、lstm的优势

- word2vec如何进行负采样

- word2vec两种训练方法的区别，具体损失函数

- 介绍LSTM每一个门的具体操作，一个LSTM cell的时间复杂度是多少

- transformer中encoder和decoder的输入分别是什么

- transformer中encoder与decoder的QKV矩阵如何产生

- transformer中QKV矩阵是否可以设置成同一个

- transformer与bert的位置编码有什么区别

- BERT中计算attention的公式

- BERT中LayerNorm的作用，为什么不用BN？

- BERT中的两种预训练任务介绍

- 深度学习中BN的好处？最早提出BN是为了解决什么问题？BN具体怎么实现的

- 激活函数中，sigmoid，tanh有什么不好的地方？relu有什么优势？

## 3.1.3 特征工程
- 特征工程一般怎么做

- 特征数值分布比较稀疏如何处理

- 正负样本不均衡如何处理

- 连续特征离散化的作用

- 对id类特征onehot导致维度过高，如何处理？

- 如何进行特征筛选

## 3.1.4 评估指标

- auc的含义和计算方法

- 如果对负样本进行采样，auc的计算结果会发生变化吗

- 交叉熵跟MSE有什么区别

- micro-f1解释

- 介绍下排序指标ndcg